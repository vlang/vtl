var searchModuleIndex = [
"README",
"autograd",
"datasets",
"la",
"ml.metrics",
"nn.gates.activation",
"nn.gates.layers",
"nn.gates.loss",
"nn.internal",
"nn.layers",
"nn.loss",
"nn.models",
"nn.optimizers",
"nn.types",
"vtl",
"stats",
"storage",
];

var searchIndex = [
"Context",
"ctx",
"Context[T]",
"len",
"push",
"last",
"pop",
"variable",
"str",
"ContextVariableData",
"register",
"CacheParam",
"Gate",
"gate_backward",
"gate_cache",
"AddGate",
"add_gate",
"AddGate[T]",
"backward",
"cache",
"SubstractGate",
"subtract_gate",
"SubstractGate[T]",
"backward",
"cache",
"MultiplyGate",
"multiply_gate",
"MultiplyGate[T]",
"backward",
"cache",
"DivideGate",
"divide_gate",
"DivideGate[T]",
"backward",
"cache",
"MatMulGate",
"matmul_gate",
"MatMulGate[T]",
"backward",
"cache",
"ExpGate",
"exp_gate",
"ExpGate[T]",
"backward",
"cache",
"PowGate",
"pow_gate",
"PowGate[T]",
"backward",
"cache",
"SinGate",
"sin_gate",
"SinGate[T]",
"backward",
"cache",
"CosGate",
"cos_gate",
"CosGate[T]",
"backward",
"cache",
"TanGate",
"tan_gate",
"TanGate[T]",
"backward",
"cache",
"Node",
"node",
"Payload",
"payload",
"Variable",
"VariableData",
"variable",
"Variable[T]",
"add",
"backprop",
"cos",
"divide",
"exp",
"is_grad_needed",
"matmul",
"multiply",
"pow",
"sin",
"slice",
"slice_hilo",
"str",
"subtract",
"tan",
"ImdbDataset",
"load_imdb",
"MnistDataset",
"load_mnist",
"dot",
"det",
"inv",
"matmul",
"accuracy_score",
"squared_error",
"mean_squared_error",
"relative_error",
"mean_relative_error",
"absolute_error",
"mean_absolute_error",
"EluGate",
"elu_gate",
"EluGate[T]",
"backward",
"cache",
"LeakyReluGate",
"leaky_relu_gate",
"LeakyReluGate[T]",
"backward",
"cache",
"ReLUGate",
"relu_gate",
"ReLUGate[T]",
"backward",
"cache",
"SigmoidGate",
"sigmoid_gate",
"SigmoidGate[T]",
"backward",
"cache",
"DropoutGate",
"dropout_gate",
"DropoutGate[T]",
"backward",
"cache",
"FlattenGate",
"flatten_gate",
"FlattenGate[T]",
"backward",
"cache",
"InputGate",
"input_gate",
"InputGate[T]",
"backward",
"cache",
"LinearGate",
"linear_gate",
"LinearGate[T]",
"backward",
"cache",
"MaxPool2DGate",
"maxpool2d_gate",
"MaxPool2DGate[T]",
"backward",
"cache",
"MseGate",
"mse_gate",
"MseGate[T]",
"backward",
"cache",
"SigmoidCrossEntropyGate",
"sigmoid_cross_entropy_gate",
"SigmoidCrossEntropyGate[T]",
"backward",
"cache",
"SoftmaxCrossEntropyGate",
"softmax_cross_entropy_gate",
"SoftmaxCrossEntropyGate[T]",
"backward",
"cache",
"tanh",
"deriv_tanh",
"sigmoid",
"deriv_sigmoid",
"relu",
"deriv_relu",
"leaky_relu",
"deriv_leaky_relu",
"elu",
"deriv_elu",
"sigmoid_cross_entropy",
"mse",
"FanMode",
"Distribution",
"compute_fans",
"variance_scaled",
"kaiming_uniform",
"kaiming_normal",
"dropout",
"dropout_backwards",
"maxpool2d",
"maxpool2d_backward",
"mse_backward",
"sigmoid_cross_entropy_backward",
"softmax_cross_entropy_backward",
"sgd_optimize",
"DropoutLayerConfig",
"DropoutLayer",
"dropout_layer",
"DropoutLayer[T]",
"output_shape",
"variables",
"forward",
"EluLayerConfig",
"EluLayer",
"elu_layer",
"EluLayer[T]",
"output_shape",
"variables",
"forward",
"FlattenLayer",
"flatten_layer",
"FlattenLayer[T]",
"output_shape",
"variables",
"forward",
"InputLayer",
"input_layer",
"InputLayer[T]",
"output_shape",
"variables",
"forward",
"LeakyReluLayer",
"leaky_relu_layer",
"LeakyReluLayer[T]",
"output_shape",
"variables",
"forward",
"LinearLayer",
"linear_layer",
"LinearLayer[T]",
"output_shape",
"variables",
"forward",
"MaxPool2DLayer",
"maxpool2d_layer",
"MaxPool2DLayer[T]",
"output_shape",
"variables",
"forward",
"ReLULayer",
"relu_layer",
"ReLULayer[T]",
"output_shape",
"variables",
"forward",
"SigmoidLayer",
"sigmoid_layer",
"SigmoidLayer[T]",
"output_shape",
"variables",
"forward",
"loss_loss",
"MSELoss",
"mse_loss",
"MSELoss[T]",
"loss",
"SigmoidCrossEntropyLoss",
"sigmoid_cross_entropy_loss",
"SigmoidCrossEntropyLoss[T]",
"loss",
"SoftmaxCrossEntropyLoss",
"softmax_cross_entropy_loss",
"SoftmaxCrossEntropyLoss[T]",
"loss",
"Sequential",
"sequential",
"sequential_with_layers",
"sequential_from_ctx",
"sequential_from_ctx_with_layers",
"Sequential[T]",
"input",
"linear",
"maxpool2d",
"mse_loss",
"sigmoid_cross_entropy_loss",
"softmax_cross_entropy_loss",
"flatten",
"relu",
"leaky_relu",
"elu",
"sigmod",
"forward",
"loss",
"SequentialInfo",
"sequential_info",
"SequentialInfo[T]",
"input",
"linear",
"maxpool2d",
"mse_loss",
"sigmoid_cross_entropy_loss",
"softmax_cross_entropy_loss",
"flatten",
"relu",
"leaky_relu",
"elu",
"sigmod",
"AdamOptimizer",
"AdamOptimizerConfig",
"adam_optimizer",
"AdamOptimizer[T]",
"build_params",
"update",
"SgdOptimizer",
"SgdOptimizerConfig",
"sgd",
"SgdOptimizer[T]",
"build_params",
"update",
"Layer",
"Loss",
"Optimizer",
"Tensor[T]",
"abs",
"acos",
"acosh",
"add",
"add_scalar",
"alike",
"all",
"any",
"apply",
"array_equal",
"array_equiv",
"array_split",
"array_split_expl",
"as_bool",
"as_f32",
"as_f64",
"as_i16",
"as_i8",
"as_int",
"as_strided",
"as_string",
"as_u8",
"asin",
"asinh",
"assert_matrix",
"assert_square_matrix",
"assign",
"atan",
"atan2",
"atanh",
"axis_iterator",
"axis_with_dims_iterator",
"broadcast_to",
"broadcastable",
"cbrt",
"ceil",
"close",
"copy",
"cos",
"cosh",
"cot",
"cpu",
"custom_iterator",
"degrees",
"diag",
"diag_flat",
"diagonal",
"divide",
"divide_scalar",
"dsplit",
"dsplit_expl",
"ensure_memory",
"equal",
"erf",
"erfc",
"exp",
"exp2",
"expand_dims",
"expm1",
"f32_bits",
"f32_from_bits",
"f64_bits",
"f64_from_bits",
"factorial",
"fill",
"floor",
"fmod",
"gamma",
"gcd",
"get",
"get_nth",
"hsplit",
"hsplit_expl",
"hypot",
"is_col_major",
"is_col_major_contiguous",
"is_contiguous",
"is_finite",
"is_inf",
"is_matrix",
"is_nan",
"is_row_major",
"is_row_major_contiguous",
"is_square_matrix",
"is_vector",
"iterator",
"iterators",
"lcm",
"log",
"log10",
"log1p",
"log2",
"log_factorial",
"log_gamma",
"log_n",
"map",
"max",
"min",
"multiply",
"multiply_scalar",
"napply",
"nextafter",
"nextafter32",
"nmap",
"not_equal",
"nreduce",
"nth_index",
"offset_index",
"pow",
"pow10",
"radians",
"rank",
"ravel",
"reduce",
"reshape",
"round",
"round_to_even",
"set",
"set_nth",
"sin",
"sinh",
"size",
"slice",
"slice_hilo",
"split",
"split_expl",
"sqrt",
"str",
"subtract",
"subtract_scalar",
"swapaxes",
"t",
"tan",
"tanh",
"to_array",
"tolerance",
"transpose",
"tril",
"tril_inpl_offset",
"tril_inplace",
"tril_offset",
"triu",
"triu_inplace",
"triu_offset",
"trunc",
"unsqueeze",
"vcl",
"veryclose",
"view",
"vsplit",
"vsplit_expl",
"with_broadcast",
"with_dims",
"broadcast2",
"broadcast3",
"broadcast_n",
"TensorData",
"from_array",
"tensor",
"tensor_like",
"tensor_like_with_shape",
"empty",
"empty_like",
"identity",
"eye",
"zeros",
"zeros_like",
"ones",
"ones_like",
"full",
"full_like",
"range",
"seq",
"from_1d",
"from_2d",
"IteratorStrategy",
"TensorIterator",
"IteratorBuildData",
"TensorIterator[T]",
"next",
"TensorsIterator",
"TensorsIterator[T]",
"next",
"TensorAxisIterator",
"TensorAxisIterator[T]",
"next",
"bernoulli",
"binomial",
"exponential",
"NormalTensorData",
"normal",
"random",
"random_seed",
"AxisData",
"vstack",
"hstack",
"dstack",
"column_stack",
"stack",
"concatenate",
"MemoryFormat",
"Tensor",
"TensorDataType",
"string",
"int",
"i64",
"i8",
"i16",
"u8",
"u16",
"u32",
"u64",
"f32",
"f64",
"bool",
"td",
"cast",
"AnyTensor",
"VclParams",
"AxisData",
"sum",
"sum_axis",
"sum_axis_with_dims",
"prod",
"prod_axis",
"prod_axis_with_dims",
"freq",
"mean",
"geometric_mean",
"harmonic_mean",
"median",
"mode",
"rms",
"population_variance",
"population_variance_mean",
"sample_variance",
"sample_variance_mean",
"population_stddev",
"population_stddev_mean",
"sample_stddev",
"sample_stddev_mean",
"absdev",
"absdev_mean",
"tss",
"tss_mean",
"min",
"max",
"minmax",
"min_index",
"max_index",
"minmax_index",
"range",
"covariance",
"covariance_mean",
"lag1_autocorrelation",
"lag1_autocorrelation_mean",
"kurtosis",
"kurtosis_mean_stddev",
"skew",
"skew_mean_stddev",
"quantile",
"CpuStorage",
"storage",
"from_array",
"CpuStorage[T]",
"get",
"set",
"fill",
"clone",
"like",
"like_with_len",
"offset",
"to_array",
];

var searchModuleData = [
["<div align=\"center\"> <p> <img style=\"width: 200px\" width=\"200\" src=\"https","index.html"],
["","autograd.html"],
["This module exposes some functionalities to download and use the VTL datasets. F","datasets.html"],
["","la.html"],
["","ml.metrics.html"],
["","nn.gates.activation.html"],
["","nn.gates.layers.html"],
["","nn.gates.loss.html"],
["","nn.internal.html"],
["","nn.layers.html"],
["","nn.loss.html"],
["","nn.models.html"],
["","nn.optimizers.html"],
["","nn.types.html"],
["<div align=\"center\"> <p> <img style=\"width: 200px\" width=\"200\" src=\"https","vtl.html"],
["","stats.html"],
["","storage.html"],
];

var searchData = [
["autograd","Context keeps track of the computational graph for a number of operations. Varia","autograd.html#Context","struct "],
["autograd","Contexts can only be initialized as empty, and a generic type must be provided","autograd.html#ctx","fn "],
["autograd","","autograd.html#Context[T]","type "],
["autograd","","autograd.html#Context[T].len","fn (Context[T])"],
["autograd","","autograd.html#Context[T].push","fn (Context[T])"],
["autograd","","autograd.html#Context[T].last","fn (Context[T])"],
["autograd","","autograd.html#Context[T].pop","fn (Context[T])"],
["autograd","","autograd.html#Context[T].variable","fn (Context[T])"],
["autograd","","autograd.html#Context[T].str","fn (Context[T])"],
["autograd","","autograd.html#ContextVariableData","struct "],
["autograd","","autograd.html#register","fn "],
["autograd","","autograd.html#CacheParam","interface "],
["autograd","Gate is an object that can cache the result of an operation, as well as backprop","autograd.html#Gate","interface "],
["autograd","","autograd.html#gate_backward","fn "],
["autograd","","autograd.html#gate_cache","fn "],
["autograd","","autograd.html#AddGate","struct "],
["autograd","","autograd.html#add_gate","fn "],
["autograd","","autograd.html#AddGate[T]","type "],
["autograd","","autograd.html#AddGate[T].backward","fn (AddGate[T])"],
["autograd","","autograd.html#AddGate[T].cache","fn (AddGate[T])"],
["autograd","","autograd.html#SubstractGate","struct "],
["autograd","","autograd.html#subtract_gate","fn "],
["autograd","","autograd.html#SubstractGate[T]","type "],
["autograd","","autograd.html#SubstractGate[T].backward","fn (SubstractGate[T])"],
["autograd","","autograd.html#SubstractGate[T].cache","fn (SubstractGate[T])"],
["autograd","","autograd.html#MultiplyGate","struct "],
["autograd","","autograd.html#multiply_gate","fn "],
["autograd","","autograd.html#MultiplyGate[T]","type "],
["autograd","","autograd.html#MultiplyGate[T].backward","fn (MultiplyGate[T])"],
["autograd","","autograd.html#MultiplyGate[T].cache","fn (MultiplyGate[T])"],
["autograd","","autograd.html#DivideGate","struct "],
["autograd","","autograd.html#divide_gate","fn "],
["autograd","","autograd.html#DivideGate[T]","type "],
["autograd","","autograd.html#DivideGate[T].backward","fn (DivideGate[T])"],
["autograd","","autograd.html#DivideGate[T].cache","fn (DivideGate[T])"],
["autograd","","autograd.html#MatMulGate","struct "],
["autograd","","autograd.html#matmul_gate","fn "],
["autograd","","autograd.html#MatMulGate[T]","type "],
["autograd","","autograd.html#MatMulGate[T].backward","fn (MatMulGate[T])"],
["autograd","","autograd.html#MatMulGate[T].cache","fn (MatMulGate[T])"],
["autograd","","autograd.html#ExpGate","struct "],
["autograd","","autograd.html#exp_gate","fn "],
["autograd","","autograd.html#ExpGate[T]","type "],
["autograd","","autograd.html#ExpGate[T].backward","fn (ExpGate[T])"],
["autograd","","autograd.html#ExpGate[T].cache","fn (ExpGate[T])"],
["autograd","","autograd.html#PowGate","struct "],
["autograd","","autograd.html#pow_gate","fn "],
["autograd","","autograd.html#PowGate[T]","type "],
["autograd","","autograd.html#PowGate[T].backward","fn (PowGate[T])"],
["autograd","","autograd.html#PowGate[T].cache","fn (PowGate[T])"],
["autograd","","autograd.html#SinGate","struct "],
["autograd","","autograd.html#sin_gate","fn "],
["autograd","","autograd.html#SinGate[T]","type "],
["autograd","","autograd.html#SinGate[T].backward","fn (SinGate[T])"],
["autograd","","autograd.html#SinGate[T].cache","fn (SinGate[T])"],
["autograd","","autograd.html#CosGate","struct "],
["autograd","","autograd.html#cos_gate","fn "],
["autograd","","autograd.html#CosGate[T]","type "],
["autograd","","autograd.html#CosGate[T].backward","fn (CosGate[T])"],
["autograd","","autograd.html#CosGate[T].cache","fn (CosGate[T])"],
["autograd","","autograd.html#TanGate","struct "],
["autograd","","autograd.html#tan_gate","fn "],
["autograd","","autograd.html#TanGate[T]","type "],
["autograd","","autograd.html#TanGate[T].backward","fn (TanGate[T])"],
["autograd","","autograd.html#TanGate[T].cache","fn (TanGate[T])"],
["autograd","Node is a member of a computational graph that contains a reference to a gate, a","autograd.html#Node","struct "],
["autograd","node","autograd.html#node","fn "],
["autograd","Payload is a simple wrapper around a Variable.  It is only abstracted out to be ","autograd.html#Payload","struct "],
["autograd","","autograd.html#payload","fn "],
["autograd","Variable is an abstraction of a vtl.Tensor that tracks the operations done to th","autograd.html#Variable","struct "],
["autograd","","autograd.html#VariableData","struct "],
["autograd","variable","autograd.html#variable","fn "],
["autograd","","autograd.html#Variable[T]","type "],
["autograd","add Adds two variables together.","autograd.html#Variable[T].add","fn (Variable[T])"],
["autograd","backprop Back propogates an operation along a computational graph. This operatio","autograd.html#Variable[T].backprop","fn (Variable[T])"],
["autograd","cos Cosine of a variable.","autograd.html#Variable[T].cos","fn (Variable[T])"],
["autograd","divide Divides two variables.","autograd.html#Variable[T].divide","fn (Variable[T])"],
["autograd","exp Exponentiates a variable.","autograd.html#Variable[T].exp","fn (Variable[T])"],
["autograd","","autograd.html#Variable[T].is_grad_needed","fn (Variable[T])"],
["autograd","matmul Multiplies two matrices.","autograd.html#Variable[T].matmul","fn (Variable[T])"],
["autograd","multiply Multiplies two variables.","autograd.html#Variable[T].multiply","fn (Variable[T])"],
["autograd","pow raises a variable to a power.","autograd.html#Variable[T].pow","fn (Variable[T])"],
["autograd","sin Sine of a variable.","autograd.html#Variable[T].sin","fn (Variable[T])"],
["autograd","","autograd.html#Variable[T].slice","fn (Variable[T])"],
["autograd","","autograd.html#Variable[T].slice_hilo","fn (Variable[T])"],
["autograd","","autograd.html#Variable[T].str","fn (Variable[T])"],
["autograd","subtract Subtracts two variables.","autograd.html#Variable[T].subtract","fn (Variable[T])"],
["autograd","tan Tan of a variable.","autograd.html#Variable[T].tan","fn (Variable[T])"],
["datasets","ImdbDataset is a dataset for sentiment analysis.","datasets.html#ImdbDataset","struct "],
["datasets","load_imdb loads the IMDB dataset.","datasets.html#load_imdb","fn "],
["datasets","MnistDataset is a dataset of MNIST handwritten digits.","datasets.html#MnistDataset","struct "],
["datasets","load_mnist loads the MNIST dataset.","datasets.html#load_mnist","fn "],
["la","","la.html#dot","fn "],
["la","","la.html#det","fn "],
["la","","la.html#inv","fn "],
["la","","la.html#matmul","fn "],
["ml.metrics","accuracy_score returns the proportion of correctly classified samples (as f64)","ml.metrics.html#accuracy_score","fn "],
["ml.metrics","","ml.metrics.html#squared_error","fn "],
["ml.metrics","","ml.metrics.html#mean_squared_error","fn "],
["ml.metrics","","ml.metrics.html#relative_error","fn "],
["ml.metrics","","ml.metrics.html#mean_relative_error","fn "],
["ml.metrics","","ml.metrics.html#absolute_error","fn "],
["ml.metrics","","ml.metrics.html#mean_absolute_error","fn "],
["nn.gates.activation","","nn.gates.activation.html#EluGate","struct "],
["nn.gates.activation","","nn.gates.activation.html#elu_gate","fn "],
["nn.gates.activation","","nn.gates.activation.html#EluGate[T]","type "],
["nn.gates.activation","","nn.gates.activation.html#EluGate[T].backward","fn (EluGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#EluGate[T].cache","fn (EluGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#LeakyReluGate","struct "],
["nn.gates.activation","","nn.gates.activation.html#leaky_relu_gate","fn "],
["nn.gates.activation","","nn.gates.activation.html#LeakyReluGate[T]","type "],
["nn.gates.activation","","nn.gates.activation.html#LeakyReluGate[T].backward","fn (LeakyReluGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#LeakyReluGate[T].cache","fn (LeakyReluGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#ReLUGate","struct "],
["nn.gates.activation","","nn.gates.activation.html#relu_gate","fn "],
["nn.gates.activation","","nn.gates.activation.html#ReLUGate[T]","type "],
["nn.gates.activation","","nn.gates.activation.html#ReLUGate[T].backward","fn (ReLUGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#ReLUGate[T].cache","fn (ReLUGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#SigmoidGate","struct "],
["nn.gates.activation","","nn.gates.activation.html#sigmoid_gate","fn "],
["nn.gates.activation","","nn.gates.activation.html#SigmoidGate[T]","type "],
["nn.gates.activation","","nn.gates.activation.html#SigmoidGate[T].backward","fn (SigmoidGate[T])"],
["nn.gates.activation","","nn.gates.activation.html#SigmoidGate[T].cache","fn (SigmoidGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#DropoutGate","struct "],
["nn.gates.layers","","nn.gates.layers.html#dropout_gate","fn "],
["nn.gates.layers","","nn.gates.layers.html#DropoutGate[T]","type "],
["nn.gates.layers","","nn.gates.layers.html#DropoutGate[T].backward","fn (DropoutGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#DropoutGate[T].cache","fn (DropoutGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#FlattenGate","struct "],
["nn.gates.layers","","nn.gates.layers.html#flatten_gate","fn "],
["nn.gates.layers","","nn.gates.layers.html#FlattenGate[T]","type "],
["nn.gates.layers","","nn.gates.layers.html#FlattenGate[T].backward","fn (FlattenGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#FlattenGate[T].cache","fn (FlattenGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#InputGate","struct "],
["nn.gates.layers","","nn.gates.layers.html#input_gate","fn "],
["nn.gates.layers","","nn.gates.layers.html#InputGate[T]","type "],
["nn.gates.layers","","nn.gates.layers.html#InputGate[T].backward","fn (InputGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#InputGate[T].cache","fn (InputGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#LinearGate","struct "],
["nn.gates.layers","","nn.gates.layers.html#linear_gate","fn "],
["nn.gates.layers","","nn.gates.layers.html#LinearGate[T]","type "],
["nn.gates.layers","","nn.gates.layers.html#LinearGate[T].backward","fn (LinearGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#LinearGate[T].cache","fn (LinearGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#MaxPool2DGate","struct "],
["nn.gates.layers","","nn.gates.layers.html#maxpool2d_gate","fn "],
["nn.gates.layers","","nn.gates.layers.html#MaxPool2DGate[T]","type "],
["nn.gates.layers","","nn.gates.layers.html#MaxPool2DGate[T].backward","fn (MaxPool2DGate[T])"],
["nn.gates.layers","","nn.gates.layers.html#MaxPool2DGate[T].cache","fn (MaxPool2DGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#MseGate","struct "],
["nn.gates.loss","","nn.gates.loss.html#mse_gate","fn "],
["nn.gates.loss","","nn.gates.loss.html#MseGate[T]","type "],
["nn.gates.loss","","nn.gates.loss.html#MseGate[T].backward","fn (MseGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#MseGate[T].cache","fn (MseGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#SigmoidCrossEntropyGate","struct "],
["nn.gates.loss","","nn.gates.loss.html#sigmoid_cross_entropy_gate","fn "],
["nn.gates.loss","","nn.gates.loss.html#SigmoidCrossEntropyGate[T]","type "],
["nn.gates.loss","","nn.gates.loss.html#SigmoidCrossEntropyGate[T].backward","fn (SigmoidCrossEntropyGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#SigmoidCrossEntropyGate[T].cache","fn (SigmoidCrossEntropyGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#SoftmaxCrossEntropyGate","struct "],
["nn.gates.loss","","nn.gates.loss.html#softmax_cross_entropy_gate","fn "],
["nn.gates.loss","","nn.gates.loss.html#SoftmaxCrossEntropyGate[T]","type "],
["nn.gates.loss","","nn.gates.loss.html#SoftmaxCrossEntropyGate[T].backward","fn (SoftmaxCrossEntropyGate[T])"],
["nn.gates.loss","","nn.gates.loss.html#SoftmaxCrossEntropyGate[T].cache","fn (SoftmaxCrossEntropyGate[T])"],
["nn.internal","tanh squashes a real-valued number to the range [-1, 1]","nn.internal.html#tanh","fn "],
["nn.internal","deriv_tanh computes the derivative of tanh","nn.internal.html#deriv_tanh","fn "],
["nn.internal","sigmoid takes a real-valued number and squashes it to the range [0, 1]","nn.internal.html#sigmoid","fn "],
["nn.internal","deriv_sigmoid computes the derivative of sigmoid","nn.internal.html#deriv_sigmoid","fn "],
["nn.internal","relu activation function","nn.internal.html#relu","fn "],
["nn.internal","deriv_relu computes the derivate of relu","nn.internal.html#deriv_relu","fn "],
["nn.internal","leaky_relu activation function","nn.internal.html#leaky_relu","fn "],
["nn.internal","deriv_leaky_relu computes the derivative of leaky_relu","nn.internal.html#deriv_leaky_relu","fn "],
["nn.internal","elu activation function","nn.internal.html#elu","fn "],
["nn.internal","deriv_elu computes the derivative of elu","nn.internal.html#deriv_elu","fn "],
["nn.internal","sigmoid_cross_entropy computes the sigmoid cross entropy between the labels and ","nn.internal.html#sigmoid_cross_entropy","fn "],
["nn.internal","mse squared error between the labels and the predictions","nn.internal.html#mse","fn "],
["nn.internal","","nn.internal.html#FanMode","enum "],
["nn.internal","","nn.internal.html#Distribution","enum "],
["nn.internal","","nn.internal.html#compute_fans","fn "],
["nn.internal","","nn.internal.html#variance_scaled","fn "],
["nn.internal","","nn.internal.html#kaiming_uniform","fn "],
["nn.internal","","nn.internal.html#kaiming_normal","fn "],
["nn.internal","","nn.internal.html#dropout","fn "],
["nn.internal","","nn.internal.html#dropout_backwards","fn "],
["nn.internal","","nn.internal.html#maxpool2d","fn "],
["nn.internal","","nn.internal.html#maxpool2d_backward","fn "],
["nn.internal","","nn.internal.html#mse_backward","fn "],
["nn.internal","","nn.internal.html#sigmoid_cross_entropy_backward","fn "],
["nn.internal","","nn.internal.html#softmax_cross_entropy_backward","fn "],
["nn.internal","","nn.internal.html#sgd_optimize","fn "],
["nn.layers","","nn.layers.html#DropoutLayerConfig","struct "],
["nn.layers","DropoutLayer is a dropout layer.","nn.layers.html#DropoutLayer","struct "],
["nn.layers","","nn.layers.html#dropout_layer","fn "],
["nn.layers","","nn.layers.html#DropoutLayer[T]","type "],
["nn.layers","","nn.layers.html#DropoutLayer[T].output_shape","fn (DropoutLayer[T])"],
["nn.layers","","nn.layers.html#DropoutLayer[T].variables","fn (DropoutLayer[T])"],
["nn.layers","","nn.layers.html#DropoutLayer[T].forward","fn (DropoutLayer[T])"],
["nn.layers","","nn.layers.html#EluLayerConfig","struct "],
["nn.layers","EluLayer is an activation layer that applies the element-wise function `f(x) = x","nn.layers.html#EluLayer","struct "],
["nn.layers","","nn.layers.html#elu_layer","fn "],
["nn.layers","","nn.layers.html#EluLayer[T]","type "],
["nn.layers","","nn.layers.html#EluLayer[T].output_shape","fn (EluLayer[T])"],
["nn.layers","","nn.layers.html#EluLayer[T].variables","fn (EluLayer[T])"],
["nn.layers","","nn.layers.html#EluLayer[T].forward","fn (EluLayer[T])"],
["nn.layers","FlattenLayer is a layer","nn.layers.html#FlattenLayer","struct "],
["nn.layers","","nn.layers.html#flatten_layer","fn "],
["nn.layers","","nn.layers.html#FlattenLayer[T]","type "],
["nn.layers","","nn.layers.html#FlattenLayer[T].output_shape","fn (FlattenLayer[T])"],
["nn.layers","","nn.layers.html#FlattenLayer[T].variables","fn (FlattenLayer[T])"],
["nn.layers","","nn.layers.html#FlattenLayer[T].forward","fn (FlattenLayer[T])"],
["nn.layers","InputLayer is a layer that takes a single input tensor and returns the same tens","nn.layers.html#InputLayer","struct "],
["nn.layers","","nn.layers.html#input_layer","fn "],
["nn.layers","","nn.layers.html#InputLayer[T]","type "],
["nn.layers","","nn.layers.html#InputLayer[T].output_shape","fn (InputLayer[T])"],
["nn.layers","","nn.layers.html#InputLayer[T].variables","fn (InputLayer[T])"],
["nn.layers","","nn.layers.html#InputLayer[T].forward","fn (InputLayer[T])"],
["nn.layers","LeakyReluLayer is an activation layer that applies the leaky elu function to the","nn.layers.html#LeakyReluLayer","struct "],
["nn.layers","","nn.layers.html#leaky_relu_layer","fn "],
["nn.layers","","nn.layers.html#LeakyReluLayer[T]","type "],
["nn.layers","","nn.layers.html#LeakyReluLayer[T].output_shape","fn (LeakyReluLayer[T])"],
["nn.layers","","nn.layers.html#LeakyReluLayer[T].variables","fn (LeakyReluLayer[T])"],
["nn.layers","","nn.layers.html#LeakyReluLayer[T].forward","fn (LeakyReluLayer[T])"],
["nn.layers","LinearLayer is a layer that applies a linear transformation to its input.","nn.layers.html#LinearLayer","struct "],
["nn.layers","","nn.layers.html#linear_layer","fn "],
["nn.layers","","nn.layers.html#LinearLayer[T]","type "],
["nn.layers","","nn.layers.html#LinearLayer[T].output_shape","fn (LinearLayer[T])"],
["nn.layers","","nn.layers.html#LinearLayer[T].variables","fn (LinearLayer[T])"],
["nn.layers","","nn.layers.html#LinearLayer[T].forward","fn (LinearLayer[T])"],
["nn.layers","MaxPool2DLayer is a layer that implements the maxpooling operation.","nn.layers.html#MaxPool2DLayer","struct "],
["nn.layers","","nn.layers.html#maxpool2d_layer","fn "],
["nn.layers","","nn.layers.html#MaxPool2DLayer[T]","type "],
["nn.layers","","nn.layers.html#MaxPool2DLayer[T].output_shape","fn (MaxPool2DLayer[T])"],
["nn.layers","","nn.layers.html#MaxPool2DLayer[T].variables","fn (MaxPool2DLayer[T])"],
["nn.layers","","nn.layers.html#MaxPool2DLayer[T].forward","fn (MaxPool2DLayer[T])"],
["nn.layers","ReLULayer is a layer that applies the rectified linear unit function element-wis","nn.layers.html#ReLULayer","struct "],
["nn.layers","","nn.layers.html#relu_layer","fn "],
["nn.layers","","nn.layers.html#ReLULayer[T]","type "],
["nn.layers","","nn.layers.html#ReLULayer[T].output_shape","fn (ReLULayer[T])"],
["nn.layers","","nn.layers.html#ReLULayer[T].variables","fn (ReLULayer[T])"],
["nn.layers","","nn.layers.html#ReLULayer[T].forward","fn (ReLULayer[T])"],
["nn.layers","SigmoidLayer is a layer that applies the sigmoid function to its input.","nn.layers.html#SigmoidLayer","struct "],
["nn.layers","","nn.layers.html#sigmoid_layer","fn "],
["nn.layers","","nn.layers.html#SigmoidLayer[T]","type "],
["nn.layers","","nn.layers.html#SigmoidLayer[T].output_shape","fn (SigmoidLayer[T])"],
["nn.layers","","nn.layers.html#SigmoidLayer[T].variables","fn (SigmoidLayer[T])"],
["nn.layers","","nn.layers.html#SigmoidLayer[T].forward","fn (SigmoidLayer[T])"],
["nn.loss","","nn.loss.html#loss_loss","fn "],
["nn.loss","MSELoss","nn.loss.html#MSELoss","struct "],
["nn.loss","","nn.loss.html#mse_loss","fn "],
["nn.loss","","nn.loss.html#MSELoss[T]","type "],
["nn.loss","","nn.loss.html#MSELoss[T].loss","fn (MSELoss[T])"],
["nn.loss","SigmoidCrossEntropyLoss","nn.loss.html#SigmoidCrossEntropyLoss","struct "],
["nn.loss","","nn.loss.html#sigmoid_cross_entropy_loss","fn "],
["nn.loss","","nn.loss.html#SigmoidCrossEntropyLoss[T]","type "],
["nn.loss","","nn.loss.html#SigmoidCrossEntropyLoss[T].loss","fn (SigmoidCrossEntropyLoss[T])"],
["nn.loss","SoftmaxCrossEntropyLoss","nn.loss.html#SoftmaxCrossEntropyLoss","struct "],
["nn.loss","","nn.loss.html#softmax_cross_entropy_loss","fn "],
["nn.loss","","nn.loss.html#SoftmaxCrossEntropyLoss[T]","type "],
["nn.loss","","nn.loss.html#SoftmaxCrossEntropyLoss[T].loss","fn (SoftmaxCrossEntropyLoss[T])"],
["nn.models","","nn.models.html#Sequential","struct "],
["nn.models","sequential creates a new sequential network with a new context.","nn.models.html#sequential","fn "],
["nn.models","sequential_with_layers creates a new sequential network with a new context and t","nn.models.html#sequential_with_layers","fn "],
["nn.models","sequential_from_ctx creates a new sequential network with the given context.","nn.models.html#sequential_from_ctx","fn "],
["nn.models","sequential_from_ctx_with_layers creates a new sequential network with the given ","nn.models.html#sequential_from_ctx_with_layers","fn "],
["nn.models","","nn.models.html#Sequential[T]","type "],
["nn.models","input adds a new input layer to the network with the given shape.","nn.models.html#Sequential[T].input","fn (Sequential[T])"],
["nn.models","linear adds a new linear layer to the network with the given output size","nn.models.html#Sequential[T].linear","fn (Sequential[T])"],
["nn.models","maxpool2d adds a new maxpool2d layer to the network with the given kernel size a","nn.models.html#Sequential[T].maxpool2d","fn (Sequential[T])"],
["nn.models","mse_loss sets the loss function to the mean squared error loss.","nn.models.html#Sequential[T].mse_loss","fn (Sequential[T])"],
["nn.models","sigmoid_cross_entropy_loss sets the loss function to the sigmoid cross entropy l","nn.models.html#Sequential[T].sigmoid_cross_entropy_loss","fn (Sequential[T])"],
["nn.models","softmax_cross_entropy_loss sets the loss function to the softmax cross entropy l","nn.models.html#Sequential[T].softmax_cross_entropy_loss","fn (Sequential[T])"],
["nn.models","flatten adds a new flatten layer to the network.","nn.models.html#Sequential[T].flatten","fn (Sequential[T])"],
["nn.models","relu adds a new relu layer to the network.","nn.models.html#Sequential[T].relu","fn (Sequential[T])"],
["nn.models","leaky_relu adds a new leaky_relu layer to the network.","nn.models.html#Sequential[T].leaky_relu","fn (Sequential[T])"],
["nn.models","elu adds a new elu layer to the network.","nn.models.html#Sequential[T].elu","fn (Sequential[T])"],
["nn.models","sigmod adds a new sigmod layer to the network.","nn.models.html#Sequential[T].sigmod","fn (Sequential[T])"],
["nn.models","","nn.models.html#Sequential[T].forward","fn (Sequential[T])"],
["nn.models","","nn.models.html#Sequential[T].loss","fn (Sequential[T])"],
["nn.models","","nn.models.html#SequentialInfo","struct "],
["nn.models","sequential_info creates a new neural network container with an empty list of lay","nn.models.html#sequential_info","fn "],
["nn.models","","nn.models.html#SequentialInfo[T]","type "],
["nn.models","input adds a new input layer to the network with the given shape.","nn.models.html#SequentialInfo[T].input","fn (SequentialInfo[T])"],
["nn.models","linear adds a new linear layer to the network with the given output size","nn.models.html#SequentialInfo[T].linear","fn (SequentialInfo[T])"],
["nn.models","maxpool2d adds a new maxpool2d layer to the network with the given kernel size a","nn.models.html#SequentialInfo[T].maxpool2d","fn (SequentialInfo[T])"],
["nn.models","mse_loss sets the loss function to the mean squared error loss.","nn.models.html#SequentialInfo[T].mse_loss","fn (SequentialInfo[T])"],
["nn.models","sigmoid_cross_entropy_loss sets the loss function to the sigmoid cross entropy l","nn.models.html#SequentialInfo[T].sigmoid_cross_entropy_loss","fn (SequentialInfo[T])"],
["nn.models","softmax_cross_entropy_loss sets the loss function to the softmax cross entropy l","nn.models.html#SequentialInfo[T].softmax_cross_entropy_loss","fn (SequentialInfo[T])"],
["nn.models","flatten adds a new flatten layer to the network.","nn.models.html#SequentialInfo[T].flatten","fn (SequentialInfo[T])"],
["nn.models","relu adds a new relu layer to the network.","nn.models.html#SequentialInfo[T].relu","fn (SequentialInfo[T])"],
["nn.models","leaky_relu adds a new leaky_relu layer to the network.","nn.models.html#SequentialInfo[T].leaky_relu","fn (SequentialInfo[T])"],
["nn.models","elu adds a new elu layer to the network.","nn.models.html#SequentialInfo[T].elu","fn (SequentialInfo[T])"],
["nn.models","sigmod adds a new sigmod layer to the network.","nn.models.html#SequentialInfo[T].sigmod","fn (SequentialInfo[T])"],
["nn.optimizers","","nn.optimizers.html#AdamOptimizer","struct "],
["nn.optimizers","","nn.optimizers.html#AdamOptimizerConfig","struct "],
["nn.optimizers","","nn.optimizers.html#adam_optimizer","fn "],
["nn.optimizers","","nn.optimizers.html#AdamOptimizer[T]","type "],
["nn.optimizers","","nn.optimizers.html#AdamOptimizer[T].build_params","fn (AdamOptimizer[T])"],
["nn.optimizers","","nn.optimizers.html#AdamOptimizer[T].update","fn (AdamOptimizer[T])"],
["nn.optimizers","","nn.optimizers.html#SgdOptimizer","struct "],
["nn.optimizers","","nn.optimizers.html#SgdOptimizerConfig","struct "],
["nn.optimizers","","nn.optimizers.html#sgd","fn "],
["nn.optimizers","","nn.optimizers.html#SgdOptimizer[T]","type "],
["nn.optimizers","","nn.optimizers.html#SgdOptimizer[T].build_params","fn (SgdOptimizer[T])"],
["nn.optimizers","","nn.optimizers.html#SgdOptimizer[T].update","fn (SgdOptimizer[T])"],
["nn.types","Layer is a generic interface for a neural network layer.","nn.types.html#Layer","interface "],
["nn.types","Loss is a generic interface for loss functions.","nn.types.html#Loss","interface "],
["nn.types","Optimizer is a generic interface for all optimizers.","nn.types.html#Optimizer","interface "],
["vtl","","vtl.html#Tensor[T]","type "],
["vtl","abs returns the elementwise abs of an tensor","vtl.html#Tensor[T].abs","fn (Tensor[T])"],
["vtl","acos returns the elementwise acos of an tensor","vtl.html#Tensor[T].acos","fn (Tensor[T])"],
["vtl","acosh returns the elementwise acosh of an tensor","vtl.html#Tensor[T].acosh","fn (Tensor[T])"],
["vtl","add adds two tensors elementwise","vtl.html#Tensor[T].add","fn (Tensor[T])"],
["vtl","add adds a scalar to a tensor elementwise","vtl.html#Tensor[T].add_scalar","fn (Tensor[T])"],
["vtl","alike compares two tensors elementwise","vtl.html#Tensor[T].alike","fn (Tensor[T])"],
["vtl","all returns whether all array elements evaluate to true.","vtl.html#Tensor[T].all","fn (Tensor[T])"],
["vtl","any returns whether any array elements evaluate to true.","vtl.html#Tensor[T].any","fn (Tensor[T])"],
["vtl","apply applies a function to each element of a given Tensor","vtl.html#Tensor[T].apply","fn (Tensor[T])"],
["vtl","array_equal returns true if input arrays have the same shape and all elements eq","vtl.html#Tensor[T].array_equal","fn (Tensor[T])"],
["vtl","array_equiv returns true if input arrays are shape consistent and all elements e","vtl.html#Tensor[T].array_equiv","fn (Tensor[T])"],
["vtl","array_split splits an array into multiple sub-arrays. Please refer to the split ","vtl.html#Tensor[T].array_split","fn (Tensor[T])"],
["vtl","array_split_expl splits an array into multiple sub-arrays. Please refer to the s","vtl.html#Tensor[T].array_split_expl","fn (Tensor[T])"],
["vtl","as_bool casts the Tensor to a Tensor of bools. If the original Tensor is not a T","vtl.html#Tensor[T].as_bool","fn (Tensor[T])"],
["vtl","as_f32 casts the Tensor to a Tensor of f32s. If the original Tensor is not a Ten","vtl.html#Tensor[T].as_f32","fn (Tensor[T])"],
["vtl","as_f64 casts the Tensor to a Tensor of f64s. If the original Tensor is not a Ten","vtl.html#Tensor[T].as_f64","fn (Tensor[T])"],
["vtl","as_i16 casts the Tensor to a Tensor of i16 values. If the original Tensor is not","vtl.html#Tensor[T].as_i16","fn (Tensor[T])"],
["vtl","as_i8 casts the Tensor to a Tensor of i8 values. If the original Tensor is not a","vtl.html#Tensor[T].as_i8","fn (Tensor[T])"],
["vtl","as_int casts the Tensor to a Tensor of ints. If the original Tensor is not a Ten","vtl.html#Tensor[T].as_int","fn (Tensor[T])"],
["vtl","as_strided returns a view of the Tensor with new shape and strides","vtl.html#Tensor[T].as_strided","fn (Tensor[T])"],
["vtl","as_string casts the Tensor to a Tensor of string values. If the original Tensor ","vtl.html#Tensor[T].as_string","fn (Tensor[T])"],
["vtl","as_u8 casts the Tensor to a Tensor of u8 values. If the original Tensor is not a","vtl.html#Tensor[T].as_u8","fn (Tensor[T])"],
["vtl","asin returns the elementwise asin of an tensor","vtl.html#Tensor[T].asin","fn (Tensor[T])"],
["vtl","asinh returns the elementwise asinh of an tensor","vtl.html#Tensor[T].asinh","fn (Tensor[T])"],
["vtl","assert_square_matrix panics if the given tensor is not a matrix","vtl.html#Tensor[T].assert_matrix","fn (Tensor[T])"],
["vtl","assert_square_matrix panics if the given tensor is not a square matrix","vtl.html#Tensor[T].assert_square_matrix","fn (Tensor[T])"],
["vtl","assign sets the values of an Tensor equal to the values of another Tensor of the","vtl.html#Tensor[T].assign","fn (Tensor[T])"],
["vtl","atan returns the elementwise atan of an tensor","vtl.html#Tensor[T].atan","fn (Tensor[T])"],
["vtl","atan2 returns the atan2 elementwise of two tensors","vtl.html#Tensor[T].atan2","fn (Tensor[T])"],
["vtl","atanh returns the elementwise atanh of an tensor","vtl.html#Tensor[T].atanh","fn (Tensor[T])"],
["vtl","axis_iterator returns an iterator over the axis of a Tensor, commonly used for r","vtl.html#Tensor[T].axis_iterator","fn (Tensor[T])"],
["vtl","axis returns an iterator over the axis of a Tensor, commonly used for reduction ","vtl.html#Tensor[T].axis_with_dims_iterator","fn (Tensor[T])"],
["vtl","broadcast_to broadcasts a Tensor to a compatible shape with no data copy","vtl.html#Tensor[T].broadcast_to","fn (Tensor[T])"],
["vtl","broadcastable takes two Tensors and either returns a valid broadcastable shape o","vtl.html#Tensor[T].broadcastable","fn (Tensor[T])"],
["vtl","cbrt returns the elementwise cbrt of an tensor","vtl.html#Tensor[T].cbrt","fn (Tensor[T])"],
["vtl","ceil returns the elementwise ceil of an tensor","vtl.html#Tensor[T].ceil","fn (Tensor[T])"],
["vtl","close compares two tensors elementwise","vtl.html#Tensor[T].close","fn (Tensor[T])"],
["vtl","copy returns a copy of a Tensor with a particular memory layout, either row_majo","vtl.html#Tensor[T].copy","fn (Tensor[T])"],
["vtl","cos returns the elementwise cos of an tensor","vtl.html#Tensor[T].cos","fn (Tensor[T])"],
["vtl","cosh returns the elementwise cosh of an tensor","vtl.html#Tensor[T].cosh","fn (Tensor[T])"],
["vtl","cot returns the elementwise cot of an tensor","vtl.html#Tensor[T].cot","fn (Tensor[T])"],
["vtl","cpu returns a Tensor from a Tensor","vtl.html#Tensor[T].cpu","fn (Tensor[T])"],
["vtl","iterator creates an iterator through a Tensor with custom data","vtl.html#Tensor[T].custom_iterator","fn (Tensor[T])"],
["vtl","degrees returns the elementwise degrees of an tensor","vtl.html#Tensor[T].degrees","fn (Tensor[T])"],
["vtl","diag constructs a diagonal array. input must be one dimensional and will be plac","vtl.html#Tensor[T].diag","fn (Tensor[T])"],
["vtl","diag_flat constructs a diagonal array. the flattened input is placed along the d","vtl.html#Tensor[T].diag_flat","fn (Tensor[T])"],
["vtl","diagonal returns a view of the diagonal entries of a two dimensional tensor","vtl.html#Tensor[T].diagonal","fn (Tensor[T])"],
["vtl","divide divides two tensors elementwise","vtl.html#Tensor[T].divide","fn (Tensor[T])"],
["vtl","divide divides a scalar to a tensor elementwise","vtl.html#Tensor[T].divide_scalar","fn (Tensor[T])"],
["vtl","dsplit splits array into multiple sub-arrays along the 3rd axis (depth). Please ","vtl.html#Tensor[T].dsplit","fn (Tensor[T])"],
["vtl","dsplit_expl splits array into multiple sub-arrays along the 3rd axis (depth). Pl","vtl.html#Tensor[T].dsplit_expl","fn (Tensor[T])"],
["vtl","ensure_memory sets a correct memory layout to a given tensor","vtl.html#Tensor[T].ensure_memory","fn (Tensor[T])"],
["vtl","equal compares two tensors elementwise","vtl.html#Tensor[T].equal","fn (Tensor[T])"],
["vtl","erf returns the elementwise erf of an tensor","vtl.html#Tensor[T].erf","fn (Tensor[T])"],
["vtl","erfc returns the elementwise erfc of an tensor","vtl.html#Tensor[T].erfc","fn (Tensor[T])"],
["vtl","exp returns the elementwise exp of an tensor","vtl.html#Tensor[T].exp","fn (Tensor[T])"],
["vtl","exp2 returns the elementwise exp2 of an tensor","vtl.html#Tensor[T].exp2","fn (Tensor[T])"],
["vtl","expand_dims adds an axis to a Tensor in order to support broadcasting operations","vtl.html#Tensor[T].expand_dims","fn (Tensor[T])"],
["vtl","expm1 returns the elementwise expm1 of an tensor","vtl.html#Tensor[T].expm1","fn (Tensor[T])"],
["vtl","f32_bits returns the elementwise f32_bits of an tensor","vtl.html#Tensor[T].f32_bits","fn (Tensor[T])"],
["vtl","f32_from_bits returns the elementwise f32_from_bits of an tensor","vtl.html#Tensor[T].f32_from_bits","fn (Tensor[T])"],
["vtl","f64_bits returns the elementwise f64_bits of an tensor","vtl.html#Tensor[T].f64_bits","fn (Tensor[T])"],
["vtl","f64_from_bits returns the elementwise f64_from_bits of an tensor","vtl.html#Tensor[T].f64_from_bits","fn (Tensor[T])"],
["vtl","factorial returns the elementwise factorial of an tensor","vtl.html#Tensor[T].factorial","fn (Tensor[T])"],
["vtl","fill fills an entire Tensor with a given value","vtl.html#Tensor[T].fill","fn (Tensor[T])"],
["vtl","floor returns the elementwise floor of an tensor","vtl.html#Tensor[T].floor","fn (Tensor[T])"],
["vtl","fmod returns the fmod elementwise of two tensors","vtl.html#Tensor[T].fmod","fn (Tensor[T])"],
["vtl","gamma returns the elementwise gamma of an tensor","vtl.html#Tensor[T].gamma","fn (Tensor[T])"],
["vtl","gcd returns the gcd elementwise of two tensors","vtl.html#Tensor[T].gcd","fn (Tensor[T])"],
["vtl","get returns a scalar value from a Tensor at the provided index","vtl.html#Tensor[T].get","fn (Tensor[T])"],
["vtl","get_nth returns a scalar value from a Tensor at the provided index","vtl.html#Tensor[T].get_nth","fn (Tensor[T])"],
["vtl","hsplit splits an array into multiple sub-arrays horizontally (column-wise). Plea","vtl.html#Tensor[T].hsplit","fn (Tensor[T])"],
["vtl","hsplit_expl splits an array into multiple sub-arrays horizontally (column-wise) ","vtl.html#Tensor[T].hsplit_expl","fn (Tensor[T])"],
["vtl","hypot returns the hypot elementwise of two tensors","vtl.html#Tensor[T].hypot","fn (Tensor[T])"],
["vtl","is_col_major returns if a Tensor is supposed to store its data in Col-Major orde","vtl.html#Tensor[T].is_col_major","fn (Tensor[T])"],
["vtl","is_col_major verifies if a Tensor stores its data in Col-Major order","vtl.html#Tensor[T].is_col_major_contiguous","fn (Tensor[T])"],
["vtl","is_contiguous verifies that a Tensor is contiguous independent of memory layout","vtl.html#Tensor[T].is_contiguous","fn (Tensor[T])"],
["vtl","is_finite returns true where x is not positive infinity, negative infinity, or N","vtl.html#Tensor[T].is_finite","fn (Tensor[T])"],
["vtl","is_inf reports whether t is an infinity, according to sign. If sign > 0, is_inf ","vtl.html#Tensor[T].is_inf","fn (Tensor[T])"],
["vtl","is_matrix returns if a Tensor is a nxm matrix or not","vtl.html#Tensor[T].is_matrix","fn (Tensor[T])"],
["vtl","is_nan reports whether f is an IEEE 754 ``not-a-number'' value.","vtl.html#Tensor[T].is_nan","fn (Tensor[T])"],
["vtl","is_row_major returns if a Tensor is supposed to store its data in Row-Major orde","vtl.html#Tensor[T].is_row_major","fn (Tensor[T])"],
["vtl","is_row_major verifies if a Tensor stores its data in Row-Major order","vtl.html#Tensor[T].is_row_major_contiguous","fn (Tensor[T])"],
["vtl","is_matrix returns if a Tensor is a square matrix or not","vtl.html#Tensor[T].is_square_matrix","fn (Tensor[T])"],
["vtl","is_matrix returns if a Tensor is a square 1D vector or not","vtl.html#Tensor[T].is_vector","fn (Tensor[T])"],
["vtl","iterator creates an iterator through a Tensor","vtl.html#Tensor[T].iterator","fn (Tensor[T])"],
["vtl","iterators creates an array of iterators through a list of tensors","vtl.html#Tensor[T].iterators","fn (Tensor[T])"],
["vtl","lcm returns the lcm elementwise of two tensors","vtl.html#Tensor[T].lcm","fn (Tensor[T])"],
["vtl","log returns the elementwise log of an tensor","vtl.html#Tensor[T].log","fn (Tensor[T])"],
["vtl","log10 returns the elementwise log10 of an tensor","vtl.html#Tensor[T].log10","fn (Tensor[T])"],
["vtl","log1p returns the elementwise log1p of an tensor","vtl.html#Tensor[T].log1p","fn (Tensor[T])"],
["vtl","log2 returns the elementwise log2 of an tensor","vtl.html#Tensor[T].log2","fn (Tensor[T])"],
["vtl","log_factorial returns the elementwise log_factorial of an tensor","vtl.html#Tensor[T].log_factorial","fn (Tensor[T])"],
["vtl","log_gamma returns the elementwise log_gamma of an tensor","vtl.html#Tensor[T].log_gamma","fn (Tensor[T])"],
["vtl","log_n returns the log_n elementwise of two tensors","vtl.html#Tensor[T].log_n","fn (Tensor[T])"],
["vtl","map maps a function to a given Tensor retuning a new Tensor with same shape","vtl.html#Tensor[T].map","fn (Tensor[T])"],
["vtl","max returns the max elementwise of two tensors","vtl.html#Tensor[T].max","fn (Tensor[T])"],
["vtl","min returns the min elementwise of two tensors","vtl.html#Tensor[T].min","fn (Tensor[T])"],
["vtl","multiply multiplies two tensors elementwise","vtl.html#Tensor[T].multiply","fn (Tensor[T])"],
["vtl","multiply multiplies a scalar to a tensor elementwise","vtl.html#Tensor[T].multiply_scalar","fn (Tensor[T])"],
["vtl","napply applies a function to each element of a given Tensor with params","vtl.html#Tensor[T].napply","fn (Tensor[T])"],
["vtl","nextafter returns the nextafter elementwise of two tensors","vtl.html#Tensor[T].nextafter","fn (Tensor[T])"],
["vtl","nextafter32 returns the nextafter32 elementwise of two tensors","vtl.html#Tensor[T].nextafter32","fn (Tensor[T])"],
["vtl","nmap maps a function to a given list of Tensor retuning a new Tensor with same s","vtl.html#Tensor[T].nmap","fn (Tensor[T])"],
["vtl","not_equal compares two tensors elementwise","vtl.html#Tensor[T].not_equal","fn (Tensor[T])"],
["vtl","nreduce reduces a function to a given list of Tensor retuning a new aggregated v","vtl.html#Tensor[T].nreduce","fn (Tensor[T])"],
["vtl","nth_index returns the nth index of a Tensor's shape for `n == 2` and a `shape` o","vtl.html#Tensor[T].nth_index","fn (Tensor[T])"],
["vtl","offset_index returns the index to a Tensor's data at a given index","vtl.html#Tensor[T].offset_index","fn (Tensor[T])"],
["vtl","pow returns the pow elementwise of two tensors","vtl.html#Tensor[T].pow","fn (Tensor[T])"],
["vtl","pow10 returns the elementwise pow10 of an tensor","vtl.html#Tensor[T].pow10","fn (Tensor[T])"],
["vtl","radians returns the elementwise deg2rad of an tensor","vtl.html#Tensor[T].radians","fn (Tensor[T])"],
["vtl","rank returns the number of dimensions of a given Tensor","vtl.html#Tensor[T].rank","fn (Tensor[T])"],
["vtl","ravel returns a flattened view of an Tensor if possible, otherwise a flattened c","vtl.html#Tensor[T].ravel","fn (Tensor[T])"],
["vtl","reduce reduces a function to a given Tensor retuning a new aggregated value","vtl.html#Tensor[T].reduce","fn (Tensor[T])"],
["vtl","reshape returns an Tensor with a new shape","vtl.html#Tensor[T].reshape","fn (Tensor[T])"],
["vtl","round rounds elements of an tensor elementwise","vtl.html#Tensor[T].round","fn (Tensor[T])"],
["vtl","round_to_even round_to_evens elements of an tensor elementwise","vtl.html#Tensor[T].round_to_even","fn (Tensor[T])"],
["vtl","set copies a scalar value into a Tensor at the provided index","vtl.html#Tensor[T].set","fn (Tensor[T])"],
["vtl","set_nth copies a scalar value into a Tensor at the provided offset","vtl.html#Tensor[T].set_nth","fn (Tensor[T])"],
["vtl","sin returns the elementwise sin of an tensor","vtl.html#Tensor[T].sin","fn (Tensor[T])"],
["vtl","sinh returns the elementwise sinh of an tensor","vtl.html#Tensor[T].sinh","fn (Tensor[T])"],
["vtl","size returns the number of allocated elements for a given tensor","vtl.html#Tensor[T].size","fn (Tensor[T])"],
["vtl","slice returns a tensor from a variadic list of indexing operations","vtl.html#Tensor[T].slice","fn (Tensor[T])"],
["vtl","slice_hilo returns a view of an array from a list of starting indices and a list","vtl.html#Tensor[T].slice_hilo","fn (Tensor[T])"],
["vtl","split splits an array into multiple sub-arrays. The array will be divided into N","vtl.html#Tensor[T].split","fn (Tensor[T])"],
["vtl","split_expl splits an array into multiple sub-arrays. The array will be divided i","vtl.html#Tensor[T].split_expl","fn (Tensor[T])"],
["vtl","sqrt returns the elementwise square root of an tensor","vtl.html#Tensor[T].sqrt","fn (Tensor[T])"],
["vtl","str returns the string representation of a Tensor","vtl.html#Tensor[T].str","fn (Tensor[T])"],
["vtl","subtract subtracts two tensors elementwise","vtl.html#Tensor[T].subtract","fn (Tensor[T])"],
["vtl","subtract subtracts a scalar to a tensor elementwise","vtl.html#Tensor[T].subtract_scalar","fn (Tensor[T])"],
["vtl","swapaxes returns a view of an tensor with two axes swapped.","vtl.html#Tensor[T].swapaxes","fn (Tensor[T])"],
["vtl","t returns a ful transpose of an tensor, with the axes reversed","vtl.html#Tensor[T].t","fn (Tensor[T])"],
["vtl","tan returns the elementwise tan of an tensor","vtl.html#Tensor[T].tan","fn (Tensor[T])"],
["vtl","tanh returns the elementwise tanh of an tensor","vtl.html#Tensor[T].tanh","fn (Tensor[T])"],
["vtl","to_array returns the flatten representation of a tensor in a v array storing ele","vtl.html#Tensor[T].to_array","fn (Tensor[T])"],
["vtl","tolerance compares two tensors elementwise with a given tolerance","vtl.html#Tensor[T].tolerance","fn (Tensor[T])"],
["vtl","transpose permutes the axes of an tensor in a specified order and returns a view","vtl.html#Tensor[T].transpose","fn (Tensor[T])"],
["vtl","tril computes the lower triangle of an array. returns a copy of an array with el","vtl.html#Tensor[T].tril","fn (Tensor[T])"],
["vtl","tril_inpl_offset computes the lower triangle of an array. modifies an array inpl","vtl.html#Tensor[T].tril_inpl_offset","fn (Tensor[T])"],
["vtl","tril_inplace computes the lower triangle of an array. modifies an array inplace ","vtl.html#Tensor[T].tril_inplace","fn (Tensor[T])"],
["vtl","tril_inplace computes the lower triangle of an array. returns a copy of an array","vtl.html#Tensor[T].tril_offset","fn (Tensor[T])"],
["vtl","triu computes the Upper triangle of an array. returns a copy of an array with el","vtl.html#Tensor[T].triu","fn (Tensor[T])"],
["vtl","triu_inplace computes the upper triangle of an array. modifies an array inplace ","vtl.html#Tensor[T].triu_inplace","fn (Tensor[T])"],
["vtl","triu_offset computes the upper triangle of an array. returns a copy of an array ","vtl.html#Tensor[T].triu_offset","fn (Tensor[T])"],
["vtl","trunc returns the elementwise trunc of an tensor","vtl.html#Tensor[T].trunc","fn (Tensor[T])"],
["vtl","unsqueeze adds a dimension of size one to a Tensor","vtl.html#Tensor[T].unsqueeze","fn (Tensor[T])"],
["vtl","vcl returns a VclTensor from a Tensor","vtl.html#Tensor[T].vcl","fn (Tensor[T])"],
["vtl","veryclose compares two tensors elementwise","vtl.html#Tensor[T].veryclose","fn (Tensor[T])"],
["vtl","view returns a view of a Tensor, identical to the parent but not owning its own ","vtl.html#Tensor[T].view","fn (Tensor[T])"],
["vtl","vsplit splits an array into multiple sub-arrays vertically (row-wise). Please re","vtl.html#Tensor[T].vsplit","fn (Tensor[T])"],
["vtl","vsplit_expl splits an array into multiple sub-arrays vertically (row-wise). Plea","vtl.html#Tensor[T].vsplit_expl","fn (Tensor[T])"],
["vtl","with_broadcast expands a `Tensor`s dimensions n times by broadcasting the shape ","vtl.html#Tensor[T].with_broadcast","fn (Tensor[T])"],
["vtl","with_dims returns a new Tensor adding dimensions so that it has at least `n` dim","vtl.html#Tensor[T].with_dims","fn (Tensor[T])"],
["vtl","broadcast2 broadcasts two Tensors against each other","vtl.html#broadcast2","fn "],
["vtl","broadcast3 broadcasts three Tensors against each other","vtl.html#broadcast3","fn "],
["vtl","broadcast_n broadcasts N Tensors against each other","vtl.html#broadcast_n","fn "],
["vtl","","vtl.html#TensorData","struct "],
["vtl","from_varray takes a one dimensional array of T values and coerces it into an arb","vtl.html#from_array","fn "],
["vtl","tensor allocates a Tensor onto specified device with a given data","vtl.html#tensor","fn "],
["vtl","tensor_like returns a new tensor created with similar storage properties as the ","vtl.html#tensor_like","fn "],
["vtl","tensor_like_with_shape returns a new tensor created with similar storage propert","vtl.html#tensor_like_with_shape","fn "],
["vtl","empty returns a new Tensor of given shape and type, without initializing entries","vtl.html#empty","fn "],
["vtl","empty_like returns a new Tensor of given shape and type as a given Tensor","vtl.html#empty_like","fn "],
["vtl","identity returns an array is a square array with ones on the main diagonal","vtl.html#identity","fn "],
["vtl","eye returns a 2D array with ones on the diagonal and zeros elsewhere","vtl.html#eye","fn "],
["vtl","zeros returns a new tensor of a given shape and type, filled with zeros","vtl.html#zeros","fn "],
["vtl","zeros_like returns a new Tensor of given shape and type as a given Tensor, fille","vtl.html#zeros_like","fn "],
["vtl","ones returns a new tensor of a given shape and type, filled with ones","vtl.html#ones","fn "],
["vtl","ones_like returns a new tensor of a given shape and type, filled with ones","vtl.html#ones_like","fn "],
["vtl","full returns a new tensor of a given shape and type, filled with the given value","vtl.html#full","fn "],
["vtl","full_like returns a new tensor of the same shape and type as a given Tensor fill","vtl.html#full_like","fn "],
["vtl","range returns a Tensor containing values ranging from [from, to)","vtl.html#range","fn "],
["vtl","seq returns a Tensor containing values ranging from [0, to)","vtl.html#seq","fn "],
["vtl","from_1d takes a one dimensional array of floating point values and returns a one","vtl.html#from_1d","fn "],
["vtl","from_2d takes a two dimensional array of floating point values and returns a two","vtl.html#from_2d","fn "],
["vtl","IteratorStrategy defines a function to use in order to mutate iteration position","vtl.html#IteratorStrategy","enum "],
["vtl","TensorIterator is a struct to hold a Tensors iteration state while iterating thr","vtl.html#TensorIterator","struct "],
["vtl","","vtl.html#IteratorBuildData","struct "],
["vtl","","vtl.html#TensorIterator[T]","type "],
["vtl","next calls the iteration type for a given iterator which is either flat or strid","vtl.html#TensorIterator[T].next","fn (TensorIterator[T])"],
["vtl","","vtl.html#TensorsIterator","struct "],
["vtl","","vtl.html#TensorsIterator[T]","type "],
["vtl","next calls the iteration type for a given list of iterators which is either flat","vtl.html#TensorsIterator[T].next","fn (TensorsIterator[T])"],
["vtl","TensorAxisIterator is the core iterator for axis-wise operations. Stores a copy ","vtl.html#TensorAxisIterator","struct "],
["vtl","","vtl.html#TensorAxisIterator[T]","type "],
["vtl","next calls the iteration type for a given iterator which is either flat or strid","vtl.html#TensorAxisIterator[T].next","fn (TensorAxisIterator[T])"],
["vtl","bernoulli returns a tensor of bernoulli random variables.","vtl.html#bernoulli","fn "],
["vtl","binomial returns a tensor of binomial random variables.","vtl.html#binomial","fn "],
["vtl","exponential returns a tensor of exponential random variables.","vtl.html#exponential","fn "],
["vtl","NormalTensorData is the data for a normal distribution.","vtl.html#NormalTensorData","struct "],
["vtl","normal returns a tensor of normal random variables.","vtl.html#normal","fn "],
["vtl","random returns a new Tensor of given shape and type, initialized with random num","vtl.html#random","fn "],
["vtl","","vtl.html#random_seed","fn "],
["vtl","","vtl.html#AxisData","struct "],
["vtl","vstack stack arrays in sequence vertically (row wise)","vtl.html#vstack","fn "],
["vtl","hstack stacks arrays in sequence horizontally (column wise)","vtl.html#hstack","fn "],
["vtl","dstack stacks arrays in sequence depth wise (along third axis)","vtl.html#dstack","fn "],
["vtl","column_stack stacks 1-D arrays as columns into a 2-D array.","vtl.html#column_stack","fn "],
["vtl","stack join a sequence of arrays along a new axis.","vtl.html#stack","fn "],
["vtl","concatenate concatenates two Tensors together","vtl.html#concatenate","fn "],
["vtl","MemoryFormat is a sum type that lists the possible memory layouts","vtl.html#MemoryFormat","enum "],
["vtl","Tensor is the main structure defined by VTL to manage N Dimensional data","vtl.html#Tensor","struct "],
["vtl","TensorDataType is a sum type that lists the possible types to be used to define ","vtl.html#TensorDataType","type "],
["vtl","string returns `TensorDataType` as a string.","vtl.html#TensorDataType.string","fn (TensorDataType)"],
["vtl","int uses `TensorDataType` as an integer.","vtl.html#TensorDataType.int","fn (TensorDataType)"],
["vtl","i64 uses `TensorDataType` as a 64-bit integer.","vtl.html#TensorDataType.i64","fn (TensorDataType)"],
["vtl","i8 uses `TensorDataType` as a 8-bit unsigned integer.","vtl.html#TensorDataType.i8","fn (TensorDataType)"],
["vtl","i16 uses `TensorDataType` as a 16-bit unsigned integer.","vtl.html#TensorDataType.i16","fn (TensorDataType)"],
["vtl","u8 uses `TensorDataType` as a 8-bit unsigned integer.","vtl.html#TensorDataType.u8","fn (TensorDataType)"],
["vtl","u16 uses `TensorDataType` as a 16-bit unsigned integer.","vtl.html#TensorDataType.u16","fn (TensorDataType)"],
["vtl","u32 uses `TensorDataType` as a 32-bit unsigned integer.","vtl.html#TensorDataType.u32","fn (TensorDataType)"],
["vtl","u64 uses `TensorDataType` as a 64-bit unsigned integer.","vtl.html#TensorDataType.u64","fn (TensorDataType)"],
["vtl","f32 uses `TensorDataType` as a 32-bit float.","vtl.html#TensorDataType.f32","fn (TensorDataType)"],
["vtl","f64 uses `TensorDataType` as a float.","vtl.html#TensorDataType.f64","fn (TensorDataType)"],
["vtl","bool uses `TensorDataType` as a bool","vtl.html#TensorDataType.bool","fn (TensorDataType)"],
["vtl","","vtl.html#td","fn "],
["vtl","","vtl.html#cast","fn "],
["vtl","AnyTensor is an interface that allows for any tensor to be used in the vtl libra","vtl.html#AnyTensor","interface "],
["vtl","","vtl.html#VclParams","struct "],
["stats","","stats.html#AxisData","struct "],
["stats","sum returns the sum of all elements of the given tensor","stats.html#sum","fn "],
["stats","sum_axis returns the sum of a given Tensor along a provided axis","stats.html#sum_axis","fn "],
["stats","sum_axis_dims returns the sum of a given Tensor along a provided axis with the r","stats.html#sum_axis_with_dims","fn "],
["stats","prod returns the product of all elements of the given tensor","stats.html#prod","fn "],
["stats","prod_axis_dims returns the product of a given Tensor along a provided axis with ","stats.html#prod_axis","fn "],
["stats","prod_axis_dims returns the product of a Tensor along a provided axis with the re","stats.html#prod_axis_with_dims","fn "],
["stats","Measure of Occurance Frequency of a given number Based on https://www.mathsisfun","stats.html#freq","fn "],
["stats","Measure of Central Tendancy Mean of the given input array Based on https://www.m","stats.html#mean","fn "],
["stats","Measure of Central Tendancy Geometric Mean of the given input array Based on htt","stats.html#geometric_mean","fn "],
["stats","Measure of Central Tendancy Harmonic Mean of the given input array Based on http","stats.html#harmonic_mean","fn "],
["stats","Measure of Central Tendancy Median of the given input array ( input array is ass","stats.html#median","fn "],
["stats","Measure of Central Tendancy Mode of the given input array Based on https://www.m","stats.html#mode","fn "],
["stats","Root Mean Square of the given input array Based on https://en.wikipedia.org/wiki","stats.html#rms","fn "],
["stats","Measure of Dispersion / Spread Population Variance of the given input array Base","stats.html#population_variance","fn "],
["stats","Measure of Dispersion / Spread Population Variance of the given input array Base","stats.html#population_variance_mean","fn "],
["stats","Measure of Dispersion / Spread Sample Variance of the given input array Based on","stats.html#sample_variance","fn "],
["stats","Measure of Dispersion / Spread Sample Variance of the given input array Based on","stats.html#sample_variance_mean","fn "],
["stats","Measure of Dispersion / Spread Population Standard Deviation of the given input ","stats.html#population_stddev","fn "],
["stats","Measure of Dispersion / Spread Population Standard Deviation of the given input ","stats.html#population_stddev_mean","fn "],
["stats","Measure of Dispersion / Spread Sample Standard Deviation of the given input arra","stats.html#sample_stddev","fn "],
["stats","Measure of Dispersion / Spread Sample Standard Deviation of the given input arra","stats.html#sample_stddev_mean","fn "],
["stats","Measure of Dispersion / Spread Mean Absolute Deviation of the given input array ","stats.html#absdev","fn "],
["stats","Measure of Dispersion / Spread Mean Absolute Deviation of the given input array ","stats.html#absdev_mean","fn "],
["stats","Sum of squares","stats.html#tss","fn "],
["stats","Sum of squares about the mean","stats.html#tss_mean","fn "],
["stats","Minimum of the given input array","stats.html#min","fn "],
["stats","Maximum of the given input array","stats.html#max","fn "],
["stats","Minimum and maximum of the given input array","stats.html#minmax","fn "],
["stats","Minimum of the given input array","stats.html#min_index","fn "],
["stats","Maximum of the given input array","stats.html#max_index","fn "],
["stats","Minimum and maximum of the given input array","stats.html#minmax_index","fn "],
["stats","Measure of Dispersion / Spread Range ( Maximum - Minimum ) of the given input ar","stats.html#range","fn "],
["stats","","stats.html#covariance","fn "],
["stats","Compute the covariance of a dataset using the recurrence relation","stats.html#covariance_mean","fn "],
["stats","","stats.html#lag1_autocorrelation","fn "],
["stats","Compute the lag-1 autocorrelation of a dataset using the recurrence relation","stats.html#lag1_autocorrelation_mean","fn "],
["stats","","stats.html#kurtosis","fn "],
["stats","Takes a dataset and finds the kurtosis using the fourth moment the deviations, n","stats.html#kurtosis_mean_stddev","fn "],
["stats","","stats.html#skew","fn "],
["stats","","stats.html#skew_mean_stddev","fn "],
["stats","","stats.html#quantile","fn "],
["storage","CpuStorage","storage.html#CpuStorage","struct "],
["storage","","storage.html#storage","fn "],
["storage","","storage.html#from_array","fn "],
["storage","","storage.html#CpuStorage[T]","type "],
["storage","Private function. Used to implement Storage operator","storage.html#CpuStorage[T].get","fn (CpuStorage[T])"],
["storage","Private function. Used to implement assigment to the Storage element","storage.html#CpuStorage[T].set","fn (CpuStorage[T])"],
["storage","fill fills an entire storage with a given value","storage.html#CpuStorage[T].fill","fn (CpuStorage[T])"],
["storage","clone returns an independent copy of a given Storage","storage.html#CpuStorage[T].clone","fn (CpuStorage[T])"],
["storage","like returns an independent copy of a given Storage","storage.html#CpuStorage[T].like","fn (CpuStorage[T])"],
["storage","like_with_len returns an independent copy of a given Storage","storage.html#CpuStorage[T].like_with_len","fn (CpuStorage[T])"],
["storage","","storage.html#CpuStorage[T].offset","fn (CpuStorage[T])"],
["storage","","storage.html#CpuStorage[T].to_array","fn (CpuStorage[T])"],
];

